// Discourse ambiguity model
// =========================
//
// This models a speaker who has some topic t \in T in mind
// and produces utterances to convey both incremental 
// meanings m and the overall topic t. 
// We can track both the changing content of the discourse
// revealing the Constant Entropy Rate effect as well
// as the overalll Speaker / Listener Cross-Entropy.


// var rData = [{
//   'nUtterances': 50,
//   'resultType': 'contextAware',
//   'alpha': 10,
//   'theta': 0.5,
//   'targetDistr': 'T1',
//   'world': 'a',
// }]



// getRData
// ========
// Get data passed from R. See corresponding runFn in .rmd
//
var getRData = function(key) {
  return rData[0][key]
}

var createWorld = function(val) {
  return {d:val}
}

// Env data
var nUtterances = getRData('nUtterances')
var resultType = getRData('resultType')
var alpha = getRData('alpha')
var theta = getRData('theta')
var targetDistr = getRData('targetDistr')
var world = createWorld(getRData('world'))
var RECURSION_LEVEL = getRData('recursionLevel') ? getRData('recursionLevel') : 1

// Topics
var topicNames = ['T1', 'T2', 'T3', 'T4']
var topicInitWeights = [1, 25, 25, 25]
var topicNamesDistr = Categorical({vs:topicNames, ps:topicInitWeights})
var topicNamesPrior = function() { 
  return sample(topicNamesDistr)
}

// Worlds
var worlds = [{d:'a'}, {d:'b'}, {d:'c'}, {d:'d'}]
var wordsDistr = Categorical({vs:worlds, ps:[1, 1, 1, 1]})
var worldsPrior = function() {return sample(wordsDistr)}

// Utterances
var utterances = ['a', 'b', 'c', 'd', 'x', 'y']
var utteranceWeights = [1, 1, 1, 1, 6, 6]
//var utteranceWeights = repeat(utterances.length, function(x) {1})
var utterancesDistr = Categorical({vs:utterances, ps:utteranceWeights})
var utterancesPrior = function() {return sample(utterancesDistr)}

// Topics
var T1 = Categorical({vs:worlds, ps:[0.6, 0.30, 0.15, 0.05]})
var T2 = Categorical({vs:worlds, ps:[0.30, 0.15, 0.05, 0.6]}) 
var T3 = Categorical({vs:worlds, ps:[0.15, 0.05, 0.6, 0.30]})
var T4 = Categorical({vs:worlds, ps:[0.05, 0.6, 0.30, 0.15]})
var topicDistr = {
  'T1': T1,
  'T2': T2,
  'T3': T3,
  'T4': T4
}

// General helpers
var getTopicNameDistr = function(topicName) {
  return topicDistr[topicName]
}

var litMeaning_ = function(utterance, world) {
  return utterance == world['d']
}

var litMeaning = function(utterance, world) {
  return utterance == world['d']
}

var utteranceMeaning = function(utterance) {
  if (utterance == '') {
    console.log("Error on", utterance)
  }
  var lastCharIndex = utterance.length - 1
  if (utterance.includes('a')) {
    return 'a'
  } else if (utterance.includes('b')) {
    return 'b'
  } else if (utterance.includes('c')) {
    return 'c'
  } else if (utterance.includes('d')) {
    return 'd'
  } else if (utterance.includes('x')) { // x is ambiguous between 'a' and 'd'
    return uniformDraw(['a', 'd'])
  } else if (utterance.includes('y')) { // y is ambiguous between 'b' and 'c'
    return uniformDraw(['b', 'c'])
  }
  else {
    console.log("Error on", utterance)
  }
}

// P(m|T)
var sampleWorldFromTopic = cache(function(topicName) {
  return function() {
    if (topicName == 'T1') {
      return sample(T1)
    } 
    else if (topicName == 'T2') { 
      return sample(T2)
    } 
    else if (topicName == 'T3') { 
      return sample(T3)
    } 
    else if (topicName == 'T4') { 
      return sample(T4)
    } 
    else {
      error('No topic: ', topicName)
    }
  }
})

var cost = function(utterance) {
  // Note cue cost is 0.2
  // Amb utterance cost is 0.1
  // Normal utterance cost is 0.2
  var costs = {
    'a':   0.2,
    'b':   0.2,
    'c':   0.2,
    'd':   0.2,
    'x':   0.1,
    'y':   0.1
  }
//   return costs[utterance]
  return 0
}

// RSA agents
//
var S0 = function(world) {
  Infer({
    model() {
      var utterance = utterancesPrior()
      var meaning = litMeaning(utteranceMeaning(utterance), world)
      factor(meaning ? 0 : -Infinity)
      return utterance
    }
  })
}


var L0 = cache(function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var meaning = litMeaning(utteranceMeaning(utterance), world)
      factor(meaning ? 0 : -Infinity)
      return world
    }
  })
})

var S1 = cache(function(world) {
  Infer({
    model(){
      var utterance = utterancesPrior()
      var L = L0(utterance)
      factor(alpha * (L.score(world) - cost(utterance)))
      return utterance
    }
  })
})

var L1 = cache(function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var S = S1(world)
      factor(S.score(utterance))
      return world
    }
  })
})

var S2 = cache(function(world) {
  Infer({
    model(){
      var utterance = utterancesPrior()
      var L = L1(utterance)
      factor(alpha * (L.score(world) - cost(utterance)))
      return utterance
    }
  })
})

var L2 = cache(function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var S = S2(world)
      factor(S.score(utterance))
      return world
    }
  })
})

var S3 = cache(function(world) {
  Infer({
    model(){
      var utterance = utterancesPrior()
      var L = L2(utterance)
      factor(alpha * (L.score(world) - cost(utterance)))
      return utterance
    }
  })
})

var L3 = cache(function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var S = S3(world)
      factor(S.score(utterance))
      return world
    }
  })
})

var S4 = cache(function(world) {
  Infer({
    model(){
      var utterance = utterancesPrior()
      var L = L3(utterance)
      factor(alpha * (L.score(world) - cost(utterance)))
      return utterance
    }
  })
})

  
var L4 = cache(function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var S = S3(world)
      factor(S.score(utterance))
      return world
    }
  })
})

var S5 = cache(function(world) {
  Infer({
    model(){
      var utterance = utterancesPrior()
      var L = L4(utterance)
      factor(alpha * (L.score(world) - cost(utterance)))
      return utterance
    }})
})
  

var getSpeakerModel = function(recursionLevel) {
  if (recursionLevel == 5) {
    return S5
  } else if (recursionLevel == 4) {
    return S4
  } else if (recursionLevel == 3) {
    return S3
  } else if (recursionLevel == 2) {
    return S2
  } else if (recursionLevel == 1) {
    return S1
  } else {
    error("Error, bad recursionLevel: ", recursionLevel)
  }
}

// SHelper
// ========
// Cache the conditoinal P(u|T=t) marginalizing over worlds
// where P(u|T=t) = \sum_w p_s1(u|w) * p_t(w|T=t)
// Used during listener posterior updates.
// 
// Parameters
// ----------
// T: str
//   topic name.
//
// Returns
// -------
// distr
//   Marginal distr over sample from T.
//   Note that when we have exact semantics this is
//   equivalent to p_t.
// 
var SHelper = cache(function(T){
  Infer({
    model() {
      var world = sampleWorldFromTopic(T)()
      var speaker = getSpeakerModel(RECURSION_LEVEL)
      return sample(speaker(world))
    }
  })
})

// listenerTopicDistrPosterior
// ===========================
// Get posterior probability over topics given data (utterances)
// Note (BP): this function can be problematic when sampling
// from S1 is exponential in the number of utterances.
//
var listenerTopicDistrPosterior = cache(function(observedData) {
  return Infer({method: 'enumerate'}, function() {
    var topicDistrName = topicNamesPrior()
    if (observedData != []) {
      var obsFn = function(datum){
        observe(SHelper(topicDistrName), datum)}
      mapData({data: observedData}, obsFn)
    }
    return topicDistrName
  })
})

// High-level speaker-listener agents
// ==================================

// Discourse aware listener
//
var LDiscourseAware = cache(function(utterance, data) {
  Infer({
    model() {
      // Sample a topic distr given prev utterances (data).
      var estTopicDistrName = sample(listenerTopicDistrPosterior(data))
      // Sample world given topic.
      var estWorld = sampleWorldFromTopic(estTopicDistrName)()
      var S = getSpeakerModel(RECURSION_LEVEL)(estWorld)
      factor(S.score(utterance))
      // Listener jointly reasons about (meaning, topic)
      return {'estWorld': estWorld, 'estTopicDistrName': estTopicDistrName}
    }
  })
})

// Discourse Unaware listener
//
var LDiscourseUnaware = cache(function(utterance, data) {
  Infer({
    model() {
      var estTopicDistrName = topicNamesPrior() // Note we simply sample for topics prior
      var estWorld = sampleWorldFromTopic(estTopicDistrName)()
      var S = getSpeakerModel(RECURSION_LEVEL)(estWorld)
      factor(S.score(utterance))
      return {'estWorld': estWorld, 'estTopicDistrName': estTopicDistrName}
    }
  })
})

var SDiscourseAware = cache(function(world, T, data) {
  Infer({
    model() {
      // var world = sampleWorldFromTopic(T)()
      var utterance = utterancesPrior()
      var L = LDiscourseAware(utterance, data)
      factor(alpha * (L.score({'estWorld': world, 'estTopicDistrName': T}) - cost(utterance)))
      return {'utterance': utterance}
    }
  })
})

// Unaware Speaker is equivalent to S0
var SDiscourseUnaware = cache(function(world, T, data) {
  Infer({
    model() {
      // var world = sampleWorldFromTopic(T)()
      var utterance = utterancesPrior()
      var L = LDiscourseUnaware(utterance, data)
      factor(alpha * (L.score({'estWorld': world, 'estTopicDistrName': T}) - cost(utterance)))
      return {'utterance': utterance}
    }
  })
})

// Baseline speaker only samples according to topic
var SBaseline = cache(function(world, T, data) {
  Infer({
    model() {
      var utterance = sample(S0(world))
      return {'utterance': utterance}
    }
  })
})


//
// Discourse speaker
// Note (BP): Would be interseting to think about this as a speaker agent
// 

var speakerType = getRData("resultType")
var useSpeaker = speakerType == "discourseAware" ? SDiscourseAware :
  speakerType == "discourseUnaware" ? SDiscourseUnaware : SBaseline


var d = []
var speakerFn = cache(function(T, d) {
  var world = sampleWorldFromTopic(T)()
  return d.concat([sample(useSpeaker(world, T, d))['utterance']])
})

var speakerRecurse = function(T, d, n) {
  if (n == 0) {
    return d
  } else {
    return speakerRecurse(T, speakerFn(T, d), n-1)
  }
}

//
// Helpers
//

// Listener probability assignment to topic "T1"
var getDataSizes = function(n) {
  return _.range(0, n)
}
// List of slice sizes
var dataSizes = getDataSizes(nUtterances)
// A context slice is the size of the preceding words (e.g. 0-3, 0-10, 0-n)
var contextSlices = map(function(x) {return [0, x]}, dataSizes)


// crossEntropy
// ============
// Calculate speaker / listener cross entropy.
//
var crossEntropy = function(speaker, listener, data) {
  // E_{~S(u|m,D)p(m|T=t)p(T=t)}[L(m, T|u, D)p(T|D)p(u)]
  Infer({method:'enumerate'}, function() {
    var T = targetDistr
    // p(m|t)
    var p_m_given_t = sampleWorldFromTopic(T)
    // m ~ p(m|t)
    var m = p_m_given_t()
    // S(u|m, d)
    var p_u_given_m_d = speaker(m, T, data)
    // u ~ S(u|m, d)
    var u = sample(p_u_given_m_d)['utterance']
    // p(m|u)
    var p_m_given_u = listener(u, data)
    // p(t|d)
    var p_t_given_d = listenerTopicDistrPosterior(data)
    // p(u)
    var p_u = utterancesDistr
    // console.log(m, T)
    // console.log(p_m_given_u.score({'estWorld': m, 'estTopicDistrName': T}), p_u.score(u), p_t_given_d.score(T))

    return p_m_given_u.score({'estWorld': m, 'estTopicDistrName': T}) + 
      p_u.score(u) +
      p_t_given_d.score(T)
  })
}


// KL
// ==
// Kullback-Leibler Divergence
//
var KL = function(speaker, listener, data) {
  Infer({method:'enumerate'}, function() {
    var T = targetDistr
    // p(m|t)
    var p_m_given_t = getTopicNameDistr(T)
    // m ~ p(m|t)
    var m = sample(p_m_given_t)
    // S(u|m, d)
    var p_u_given_m_d = speaker(m, T, data)
    // u ~ S(u|m, d)
    var u = sample(p_u_given_m_d)['utterance']
    // p(m|u, d)
    var p_m_given_u = listener(u, data)
    // p(t|d)
    var p_t_given_d = listenerTopicDistrPosterior(data)
    // p(u)
    var p_u = utterancesDistr
    return (p_u_given_m_d.score({'utterance': u}) + p_m_given_t.score(m)) - 
      (p_m_given_u.score({'estWorld': m, 'estTopicDistrName': T}) + p_t_given_d.score(T) + p_u.score(u))
  })
}

// Speaker cost
// =============
// \sum_{u, m}S(u|m)p(m|t)log2(p(u))
//
var speakerEffort = function(speaker, listener, data) {
  Infer({method:'enumerate'}, function() {
    var T = targetDistr
    // p(m|t)
    var p_m_given_t = sampleWorldFromTopic(T)
    // m ~ p(m|t)
    var m = p_m_given_t()
    // S(u|m, d)
    var p_u_given_m_d = speaker(m, T, data)
    // u ~ S(u|m, d)
    var u = sample(p_u_given_m_d)['utterance']
    // p(u)
    var p_u = utterancesDistr
    return p_u.score(u)
  })
}

// Listener cost
// =============
// \sum_{u, m}S(u|m)p(m|t)log2(L(m|u,d)p(T|d))
//
var listenerEffort = function(speaker, listener, data, useData=true) {
  Infer({method:'enumerate'}, function() {
    var T = targetDistr
    // p(m|t)
    var p_m_given_t = sampleWorldFromTopic(T)
    // m ~ p(m|t)
    var m = p_m_given_t()
    // S(u|m, d)
    var p_u_given_m_d = speaker(m, T, data)
    // u ~ S(u|m, d)
    var u = sample(p_u_given_m_d)['utterance']
    // p(m|u, d)
    var p_m_given_u = listener(u, data)
    // p(t|d)
    var p_t_given_d = useData ? listenerTopicDistrPosterior(data) : topicNamesDistr
    return p_m_given_u.score({'estWorld': m, 'estTopicDistrName': T}) + p_t_given_d.score(T)
      
  })
}

var trackObjective = function(objective, utterances, speaker, listener) {
  var res = map(function(x) {    
    return expectation(objective(speaker, 
                                  listener, 
                                  utterances.slice(x[0], x[1])))
  }, contextSlices)
  return res
}

// getListenerInferences
// =====================
// Given a slice of speaker utterance data, return listeners state of belief
// about the intended topic distribution.
//
var getListenerInferences = function(utterances) {
  var res = map(function(x) {
    return {
      "T1": Math.exp(listenerTopicDistrPosterior(utterances.slice(x[0], x[1])).score("T1")),
      "T2": Math.exp(listenerTopicDistrPosterior(utterances.slice(x[0], x[1])).score("T2")),
      "T3": Math.exp(listenerTopicDistrPosterior(utterances.slice(x[0], x[1])).score("T3")),
      "T4": Math.exp(listenerTopicDistrPosterior(utterances.slice(x[0], x[1])).score("T4"))}
  }, contextSlices)
  return res
}


// processUtterances
// =================
// Return a list of dictionaries containing speaker utterance data
// and corresponding state of listener inferences about topics.
//
var processUtterances = function(speakerData, 
                                  speakerType,
                                  listenerInferences, 
                                  crossEntropies,
                                  naiveCrossEntropies,
                                  kls, 
                                  naiveKLs,
                                  listenerCosts,
                                  naiveListenerCosts,
                                  speakerCosts) {
  var dMap = map(function(x) {
    return {
      'speakerType': speakerType,
      'utteranceNum': x, 
      'utterance': speakerData[x], 
      'T1': listenerInferences[x]['T1'],
      'T2': listenerInferences[x]['T2'],
      'T3': listenerInferences[x]['T3'],
      'T4': listenerInferences[x]['T4'],
      'CE': crossEntropies[x],
      'naiveCE': naiveCrossEntropies[x],
      'KL': kls[x],
      'naiveKL': naiveKLs[x],
      'listenerCosts': listenerCosts[x],
      'naiveListenerCosts': naiveListenerCosts[x],
      'speakerCosts': speakerCosts[x]}
  }, dataSizes)
  return dMap
}

// runFn
// =====
// Return data of speaker utterances / listener inferences.
//
// Parameters
// ----------
// resultType : str
//   One of (contextAwareSpeaker, contextUnawareSpeaker, baseline)
//
// Returns
// --------
// List of dicts
//
var runFn = function(speakerType) {
  var useSpeaker = speakerType == "discourseAware" ? SDiscourseAware :
    speakerType == "discourseUnaware" ? SDiscourseUnaware : SBaseline
  var utteranceList = speakerRecurse(targetDistr, [], nUtterances)
  var listenerInferences = getListenerInferences(utteranceList)
  var crossEntropies = trackObjective(crossEntropy, utteranceList, useSpeaker, LDiscourseAware)
  var naiveCrossEntropies = trackObjective(crossEntropy, utteranceList, useSpeaker, LDiscourseUnaware)
  var KLs = trackObjective(KL, utteranceList, useSpeaker, LDiscourseAware)
  var naiveKLs = trackObjective(KL, utteranceList, useSpeaker, LDiscourseUnaware)
  var listenerCosts = trackObjective(listenerEffort, utteranceList, useSpeaker, LDiscourseAware)
  var naiveListenerCosts = trackObjective(listenerEffort, utteranceList, useSpeaker, LDiscourseUnaware, false)
  var speakerCosts = trackObjective(speakerEffort, utteranceList, useSpeaker, LDiscourseAware)
  return processUtterances(utteranceList, 
    speakerType,
    listenerInferences, 
    crossEntropies, 
    naiveCrossEntropies,
    KLs, 
    naiveKLs,
    listenerCosts, 
    naiveListenerCosts,
    speakerCosts)
}

// 
// run
// Note speakerType defined line :384
runFn(speakerType)