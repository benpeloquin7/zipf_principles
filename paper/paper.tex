% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{apacite}

% KM added 1/4/18 to allow control of blind submission


\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{The interactions of rational, pragmatic agents\\
lead to efficient language structure and use}


\author{{\large \bf Benjamin N. Peloquin} \\ \texttt{bpeloqui@stanford.edu} \\ Department of Psychology \\ Stanford University \And {\large \bf Noah D. Goodman} \\ \texttt{ngoodman@stanford.edu} \\ Department of Computer Science \\ Stanford University \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@university.edu} \\ Department of Psychology \\ Stanford University}

\begin{document}

\maketitle

\begin{abstract}
Languages display a diverse set of distributional regularities such as
the relation between a word's frequency and rank in a corpus, the
distribution of syntactic dependency lengths across languages, or the
presence of properties such as ambiguity. We discuss a framework for
studying how these system-level properties emerge from local,
in-the-moment interactions of rational, pragmatic speakers and
listeners. To do so, we derive a novel objective function for measuring
the communicative efficiency of linguistic systems in terms of the
interactions of speakers and listeners. We examine the behavior of this
objective in a series of simulations focusing on the communicative
function of ambiguity in language. These simulations suggest that
rational pragmatic agents will produce communicatively efficient systems
and that interactions between such agents provide a framework for
examining efficient properties of language structure and use more
broadly.

\textbf{Keywords:}
Communicative efficiency, Rational Speech Act theory, computational
modeling, information theory, agent-based simulation
\end{abstract}

\section{Introduction}\label{introduction}

Why do languages look the way they do? Zipf (1949) proposed that
distributional properties found in natural language were evidence of
speaker-listener effort minimization. In his own words, ``we are arguing
that people do in fact act with a maximum economy of effort, and that
therefore in the process of speaking-listening they will automatically
minimize the expenditure of effort.'' Evidence for this claim has been
largely derived at the level of the lexicon. Zipf argued that the
particular relationship between a word's frequency and its rank, length,
and denotation size could be explained as an emergent property of
speaker-listener effort minimization. \par

Zipf articulated what is now considered a \emph{functionalist} approach
to language science -- analyzing language structure and use in terms of
efficiency. Such an approach might reframe our opening question as
follows: how does having property \textit{x} make using language
\(\ell\) more or less useful for communication? Among others, this
efficiency-based framing has produced a rich set of theoretical and
empirical targets exploring semantic typology (Regier, Kemp, \& Kay,
2015), properties such as ambiguity (Piantadosi, Tily, \& Gibson, 2011)
and compositionality (Kirby, Griffiths, \& Smith, 2014), and the
efficient use of reduction and redundancy in production (Genzel \&
Charniak, 2002; Levy \& Jaeger, 2007). These projects typically employ
information-theoretic principles to characterize efficiency in natural
or simulated language corpora. We adopt a different approach in this
work, deriving a measure of efficiency from first principles of
\textit{efficient language use} and show how this can lead to
communicatively efficient systems.\par

Functionalist theories commonly frame language efficiency in terms of a
fundamental effort-assymetry underlying everyday communication: what is
``hard'' for a speaker is likely different than what is ``hard'' for a
listener. Zipf described this as follows -- purely from the standpoint
of speaker effort, an optimal language \(\ell_{speaker}^*\) would tend
toward a vocabulary of a single, low-cost word. Given such a language,
the full set of potential meanings would be conveyed using only that
word, i.e. \(\ell_{speaker}^*\) would be fully ambiguous and all
possible meanings would need to be disambiguated by a listener. From the
standpoint of listener effort, an optimal language \(\ell_{listener}^*\)
would map all possible meanings to distinct words, removing a listener's
need to disambiguate. In this example, speaker effort is related to
\emph{production cost} and listener effort to \emph{understanding or
disambiguation cost}. Clearly, natural languages fall between the two
extremes of \(\ell_{speaker}^*\) and \(\ell_{listener}^*\). Zipf
proposed that the particular lexicon-level properties he observed were a
result of these competing forces -- the pressure to jointly minimize
speaker and listener effort.\par

But how does this optimization take place? The example given by Zipf
(1949), describes local, communicative interactions in terms of a
\textit{reference game}. Speakers intend to refer to some object in the
world \(m\). They choose some utterance \(u\) to transmit this intended
meaning, \(u \rightarrow m\). The listener attempts to reconstruct this
intended meaning given the transmitted utterance, \(m \rightarrow u\).
Other projects have assumed this basic reference game setting
(Piantadosi et al., 2011; Regier et al., 2015) and this simplification
of the communicative act has proven productive in theoretical
(Ferrer-i-Cancho, 2018), simulation-based (Kirby et al., 2014) and
empirical explorations (Hawkins, Franke, Smith, \& Goodman, 2018) of
efficient language structure and use.\par

Zipf's conception of speaker and listener effort may be connected to
speakers' pragmatic reasoning in the moment. Under a Gricean treatment
of pragmatics, speakers and listeners follow a set of conversational
maxims in which they cooperate to transfer information (Grice, 1975).
These maxims appear to emerge from efficiency concerns, however (Horn,
1984). We formalize this connection -- showing how system-level
efficiencies can emerge from local interaction behavior. Intuitively,
our claim is that to understand an ``efficient'' property of a system it
is essential that we consider how that property is \emph{used}
efficiently. It follows that any quantitative measure of ``efficiency''
must be a function of both the system and its users.\par

In the current work we provide a case-study for the framing presented
above. We choose a property of languages that could, in principle, vary
freely, but shows strong regularities across languages. The explanadum
is why this regularity holds. We examine ambiguity as our property,
extending ideas by Piantadosi et al. (2011). We define a novel measure
of efficiency that depends on the interactional behavior of speaker and
listener agents. We adopt the reference game as our primary unit of
interaction and model language users with the Rational Speech Act (RSA)
framework -- a computional model of language use, which is nicely
attested by experimental data on interaction. Using these basic
ingredients, we show that the property of interest (ambiguity) is
prevalent in languages that optimize our measure of efficiency. Further,
we show how ambiguity is \textit{used} efficiently during local,
in-the-moment interactions. The contributions of this work are twofold
-- we derive a novel objective for measuring linguistic efficiency and
also show how the reference game framework, in combination with formal
models of communication, can be used to connect ideas about system-level
efficiencies to in-the-moment language use.\par

\section{Exploring efficient language design and use in rational
pragmatic
agents}\label{exploring-efficient-language-design-and-use-in-rational-pragmatic-agents}

\begin{CodeChunk}
\begin{figure}[H]

{\centering \includegraphics{figs/plot-reference-game-1} 

}

\caption[An example reference game with associated literal semantics (in our terminology a ``language'')]{An example reference game with associated literal semantics (in our terminology a ``language'').}\label{fig:plot-reference-game}
\end{figure}
\end{CodeChunk}

\subsubsection{Reference games}\label{reference-games}

Zipf's example of optimal speaker- and listener-languages took the form
of a reference game. We adopt that formulation here, assuming these
communication games as our basic unit of analysis. In this framework,
speakers and listeners are aware of a set of objects \(M\)
(\emph{meanings}) and are knowledgeable about the set of possible
signals \(U\) (\emph{utterances}) that can be used to refer to a given
meaning (see Figure 1). Utterances may have different relative costs,
operationalized via a prior over utterances \(P(U)\). Similarly,
meanings differ in the relative degree to which they need to be talked
about, operationalized as a prior over meanings
\(P(M)\)\footnote{The prior over meanings are analogous to the \textit{need probabilities} assumed in previous work (Regier, Kemp \& Kay (2015).}.
We consider a set of contexts \(C\) with an associated prior \(P(C)\).
Each context \(c\in C\) describes a different distribution over meanings
e.g. \(p(M|C=c_i) \neq p(M|C=c_j)\). Finally, we consider a set of
communicative events \(e \in E\) where \(<u, m, c> = e\) is an
utterance-meaning-context triple.\par

\subsubsection{Languages}\label{languages}

A language \(\ell\) defines the set of semantic mappings between
utterance and meanings. For example, Figure 1 contains four utterances
\(U = \{\text{blue}, \text{green}, \text{square}, \text{circle}\} \text{ and three meanings }M = \{\text{green-square}, \text{blue-square}, \text{green-circle}\}\).
The boolean matrix describes the literal semantics of the language. We
define a language as ``ambiguous'' if there is some utterance
\(u \in U\) which can apply to multiple meanings (i.e.
\(|[[u_i]]| > 1\))\footnote{We use double brackets $[[\dots]]$ to represent denotation.}.
In Figure 1 both the words ``square'' and ``green'' are ambigous so we
would say that \(\ell\) contains ambiguity.

\subsubsection{Speakers and listeners}\label{speakers-and-listeners}

The Rational Speech Act framework (RSA) is a computational-level theory
of pragmatic language use, which has been well attested in experimental
work examining both speaker and listener communication behavior (Frank
\& Goodman, 2012; Goodman \& Frank, 2016). RSA is a formalization of
essential Gricean pragmatic principles -- agents reason about one
another and their shared context (Grice, 1975). We adopt RSA as our
representational framework to model Gricean (rational and pragmatic)
speaker-listeners in the reference game
setting\footnote{See SI section 1 for a more detailed description of basic RSA speaker-listener defintions and to Goodman \& Frank (2016) for an overview of its applications.}.\par

An RSA \emph{speaker agent} defines a conditional distribution over
utterances, mapping from intended meanings \(M\) to utterances \(U\)
using \(\ell\) in a given context \(c\). That is, a speaker defines
\(P_{speaker}(u|m, c; \ell)\). We will use \(S(u|m, c; \ell)\) as
short-hand throughout. A \emph{listener agent} defines a conditional
distribution over meanings, mapping from utterances \(U\) to meanings
\(M\) using \(\ell\) in a given context \(c\) (i.e. \(L(m|u, c;\ell)\)).
Note that both speakers and listeners can induce joint distributions
over utterace-meaning pairs, although, importantly, these distributions
may differ: \[P_{speaker}(u, m | c; \ell) = S(u|m, c; \ell)p(m|c)\]
\[P_{listener}(u, m| c; \ell) = L(m|u, c; \ell)p(u|c)\]

\section{Zipfian objective for linguistic system
efficiency}\label{zipfian-objective-for-linguistic-system-efficiency}

Zipf (1949) proposed that the particular distributional properties found
in natural language emerge from competing speaker and listener
pressures. We operationalize this objective in equation (1) -- the
efficiency of a linguistic system \(\ell\) being used by speaker and
listener agents \(S\) and \(L\) is the sum of the expected speaker and
listener effort to communicate over all possible communicative events
\(e \in E\).\par

\begin{equation}
\begin{split}
  \text{Efficiency}(S, L, \ell) = \mathbb{E}_{e \sim P(E)}[\text{speaker effort}] \\+ \mathbb{E}_{e \sim P(E)}[\text{listener effort}]
\end{split}
\end{equation}

We assume that speaker effort is related to the suprisal of an utterance
in a particular
context\footnote{In the current set of simulations we consider utterances costs as independent from context (ie. $p(u|c)p(c)=p(u)p(c)$).}
-- intuitively, the number of bits needed to encode the utterance \(u\).
This particular formalization of speaker-cost is general enough to
accommodate a range of cost instantiations, such as production difficult
via articulation effort, cognitive effort related to lexical access, or
others (Bennett \& Goodman, 2018).\par

\[\text{speaker effort} = -log_2(p(u|c))\]

We assume listener effort is the semantic suprisal of a meaning given an
utterance. This operationalization of listener effort is intuitively
related to existing work in sentence processing in which word
comprehension difficulty is proportional to surprisal (Hale, 2001; Levy,
2008).

\[\text{listener effort} = -log_2(L(m|u, c; \ell))\]

Importantly, we assume that events \(e = <u, m, c>\) are sampled
according the to following generative model -- some context occurs in
the world with probability \(P(C=c)\). Within this context, an object
\(m\) occurs with probability \(p(m|c)\). The speaker attempts to refer
to that object by sampling from her conditional distribution
\(S(u|m, c; \ell)\) (i.e. \(e \sim p(c)p(m|c)S(u|m, c; \ell)\)). From
the ingredients described above, it is possible to derive the following
objective between the speaker and listener joint distributions (see SI
2.2 for a complete derivation).

\begin{equation}
\begin{split}
  = \mathbb{E}_{c \sim P(C)}[H_{cross}(P_{speaker}, P_{listener} | c; \ell)]\\
\end{split}
\end{equation}

From an information-theoretic perspective this objective is intuitive:
The Cross-Entropy (CE) gives us a measure of dissimilarity between two
distributions -- the average number of bits required to communicate
under one distribution, given that the ``true'' distribution differs. In
our case, we have an expectation over this term -- the expected
difference between the distributions assumed by the speaker
\(P_{speaker}\) and listener \(P_{listener}\) given a set of contexts
\(C\)\footnote{Note that in the single context case $|C|=1$ this objective is simply the speaker-listener Cross-Entropy over utterance-meaning pairs $H_{cross}[P_{speaker}(u, m; \ell), P_{listener}(u, m; \ell)]$}.
In other words, an ``efficient'' language \(\ell\) minimizes the
distance between what speakers and listeners think.

\section{Simulating the communicative function of
ambiguity}\label{simulating-the-communicative-function-of-ambiguity}

The task of understanding language is marked by a frequent need to
handle various forms of ambiguity: lexical, syntactic, among others
(Wasow, Perfors, \& Beaver, 2005). The ubiquity of this property,
however, has been argued to provide evidence that languages have not
been optimized for
communication\footnote{Chomsky's (2002) claim that "... language is not well designed, because you have such properties as ambiguity" is analogous to the Zipfian optimal Listener language $\ell_{listener}^*$.}
(Chomsky, 2002).

Piantadosi et al. (2011) argue just the opposite, claiming that
ambiguity is an \emph{efficient} property of any communication system in
which \emph{communication is contextualized}. Simply put, it is useful
to have a language that re-uses low-cost material (has ambiguity) so
long as the cost of disambiguating the material is low. In particular,
context (or common ground) can provide useful information for
disambiguation.\par

As an example, say we have two objects (\(m_1\) and \(m_2\)), two
utterances (\(u_1\) and \(u_2\)), which differ in cost, and two
languages (\(\ell_1\) and \(\ell_2\)), which describe different
utterance-meaning mappings. In language \(\ell_1\), the low-cost \(u_1\)
can be used to refer to both \(m_1\) and \(m_2\)
(\([[u_1]]_{\ell_1} = \{m_1, m_2 \}\)), but the high-cost \(u_2\) cannot
be used at all (\([[u_2]]_{\ell_1} = \emptyset\)). By contrast, in
language \(\ell_2\), \(u_1\) can only refer to \(m_1\) and \(u_2\) can
only refer to \(m_2\) (\([[u_1]]_{\ell_2} = \{m_1\}\) and
\([[u_2]]_{\ell_2} = \{m_2 \}\)). While it is cheaper for a speaker to
use \(\ell_1\) (because speaking is always low-cost), it is more
difficult for a listener (because \(u_1\) is ambiguous). Crucially,
however, if context is disambiguating then the speaker can use \(u_1\)
to refer to either \(m_1\) or \(m_2\) and \(\ell_1\) should be preferred
to \(\ell_2\).\par

In the following experiments we explore two aspects of Piantadosi's
claim. In Simulation 1, we examine the efficient language
\emph{structure} aspect of their claim, exploring when the optimal
linguistic system \(\ell^*\) is most likely to contain ambiguous lexical
items. In Simulation 2, we explore an efficient language \emph{use}
aspect of the claim -- at what point in a conversation is it useful for
a speaker to use ambiguous lexical material?\par

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/plot-optimal-langs-1} 

}

\caption[Optimal languages are more likely to contain ambiguous items as the amount of contextual information increases]{Optimal languages are more likely to contain ambiguous items as the amount of contextual information increases. Vertical axis shows the proportion of optimal languages containing ambiguity. Horizontal axis shows the number of context-sizes (1-4). Red-line represents the optimal language under our Zipfian cross-entropy objective while the blue and red lines show optimal languages under speaker- and listener-only objectives. Error bars represent 95 percent confidence intervals.}\label{fig:plot-optimal-langs}
\end{figure*}
\end{CodeChunk}

\section{Simulation 1: Optimal languages contain ambiguity when context
is
informative}\label{simulation-1-optimal-languages-contain-ambiguity-when-context-is-informative}

We adopt the hypothesis presented by Piantadosi, formalizing their
argument in our current framework -- showing that ambiguity is an
efficient property under our CE objective in the reference game setting.
We proceed by generating languages with different amounts of contextual
support (varying the size of \(|C|\)). We search the space of languages,
examining whether ones which minimize our objective contain ambiguity.
If context leads to more efficient communication, then optimal languages
under our objective should be more likley to be ambiguous as the amount
of context increases.\par

\subsection{Simulation set-up}\label{simulation-set-up}

We conduct \(N=2000\) simulations. For each simulation we enumerate the
set of \emph{valid} languages in which \(|U|=|M|=4\) (recall that \(U\)
is our set of utterances and \(M\) our set of meanings). Note that a
language \(\ell \in L\) is ``valid'' so long as each possible meaning in
\(m \in M\) can be referred to by at least one form \(u \in U\) (every
column of \(\ell\) has some non-zero assignment) and each form maps to
at least one meaning (every column has some non-zero assignment). For a
given simulation, the goal is to find the language \(\ell^*\) which
minimizes our objective and then check to see if that language contains
ambiguity.\par

We define language efficiency as a function of the particular semantic
mappings induced by that language, the speaker and listener agents
(\(S\) and \(L\)), as well as the utterance (\(P(U)\)), meaning
(\(P(M)\)), and context priors (\(P(C)\)). Rather than assume particular
structure, for each simulation we generate
\(P(U) \sim \text{Dir}(1, |U|)\), \(P(M|C=c) \sim \text{Dir}(1, |M|)\)
(a separate conditional distribution over meanings for each context
\(c\)), and \(P(C) \sim \text{Dir}(1, |C|)\), where \(\text{Dir}(1, k)\)
specifies the uniform Dirichlet distribution over a \(k\)-dimensional
probability vector.\par

\subsubsection{Context}\label{context}

Following the argument given by Piantadosi et al. (2011), we want to
assess the impact of \emph{context} on the presence of ambiguity in
optimal languages. To do so we consider four conditions with \(n=500\)
simulations each (that is, 500 unique sets of
\(\{P(U), P(M|C), P(C)\}\). Our first is a \textit{one-context}
condition (\(|C|=1\)) -- only a single distribution over meanings
\(P(M)\). In our \textit{two-context} condition (\(|C| = 2\)), we
consider efficiency under both \(P(M|C=c_1)\) as well as \(P(M|C=c_2)\).
\textit{Three-} and \textit{four-context} conditions corresponding
accordingly.\par

\subsubsection{Baselines}\label{baselines}

For comparison, we also examine properties of optimal languages under
two additional objectives. Zipf (1949) proposed that the speaker-optimal
language \(\ell_{speaker}^*\) would minimize speaker effort and the
listener-optimal language \(\ell_{listener}^*\) would minimize listener
effort. We define these objectives using the first and second half of
equation 1 (see SI section 2.2. for more detail).\par

\subsection{Results and Discussion}\label{results-and-discussion}

In Simulation 1 we explored the degree to which ambiguity is an
efficient property of languages when communication is contextualized.
Figure 1. plots the proportion of optimal languages under each objective
as a function of context size. The red line shows that as the number of
contexts increases, so does the probability that an optimal language
\(\ell^*_{cross}\) contains ambiguity under our CE objective. For
comparison we also plot the proportion of speaker-optimal
\(\ell^*_{speaker}\) (blue line) and listener-optimal
\(\ell^*_{listener}\) (green line) languages that contain ambiguity. In
line with Zipf's predictions, if languages are designed only to minimize
speaker effort then optimal languages always contain ambiguity. If
languages are designed to minimize listener effort then ambiguity is
always avoided.\par

While our results indicate that ambiguity is an efficient property of
contextualized language, these simulations assumed that agents could
always disambiguate items in context. That is, in our four conditions
both agents had perfect knowledge of the relevant conditional
distributions (\(P(M|C)\)). This assumption may be too strong for
describing much of day-to-day communication -- we seldom interact with
others with perfect knowledge of the current context (or topic) at the
start of a conversation. To explore how ambiguity may be \textit{used}
efficiently in our framework, we next examine a case in which the
listener has imperfect knowledge of context at the start of the
conversation, but may infer it from the contents of discourse.\par

\section{Simulation 2: Rational, pragmatic speakers use ambiguity
efficiently}\label{simulation-2-rational-pragmatic-speakers-use-ambiguity-efficiently}

We shift focus in Simulation 2 -- instead of examining the landscape of
a space of languages under our CE objective, we consider a single
language containing ambiguity, examining how that property is used over
a discourse. If context is not informative with respect to meaning early
in a discourse, but is at later positions, then an efficient speaking
strategy would avoid ambiguous material early, saving it for when
contextual information is disambiguating. We can consider ``context'' as
analogous to a ``topic'' of conversation. As an example, imagine a
scenario in which a reader is beginning a magazine article. While they
may have some knowledge about the article's topic (perhaps from the
title), they may not have complete knowledge of its contents, including
the persons or events involved. In this setting, using a low-cost, but
ambiguous pronoun early may lead to misunderstanding if context is not
informative. But, if by a later position enough contextual information
has accumulated, it may be efficient to use the ambiguous material.\par

\subsection{Simulation set-up}\label{simulation-set-up-1}

We consider consider a single language \(\ell\), which contains both
ambiguous and unambiguous utterances. We assume ambiguous utterances are
lower cost. Crucially, we do not assume that the listener knows the
particular topic (\(c_{current}\)) of the conversation
\textit{a priori}. Rather, that the listener has knowledge of the set of
possible topics \(C = \{c_1, \dots, c_k\}\), but
\textit{does not know which one is currently being used by the speaker}.
Formally, this means the listener does not have access to the correct
conditional distribution over meanings \(P(M|C=c_{current})\) at the
start of the discourse.\par 

Over the course of a discourse \(D\), the listener tries to infer both
the current topic, \(c_{current}\), as well as the particular meaning
\(m\) of a given utterance \(u\). That is, we consider updated listener
and speakers (\(L(m, c|u,D;\ell)\), \(S(u|m,c,D;\ell)\)) in which both
agents can track the history of previous utterances \(D\). Importantly,
an agent can attempt to infer the current topic of conversation
\(c_{current}\) using the discourse history \(D\).\par

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/plot-optimal-use-1} 

}

\caption[(A) shows the empirical probability that our speaker used an ambiguous utterance as a function of discourse position]{(A) shows the empirical probability that our speaker used an ambiguous utterance as a function of discourse position. (B) shows speaker effort across the three models. (C) shows the Cross-Entropy objective under our three speaker models. Error bars represent 95 percent confidence intervals.}\label{fig:plot-optimal-use}
\end{figure*}
\end{CodeChunk}

We conduct \(N=600\) simulations, generating discourses of length
\(|D|=30\) utterances, comparing three speaker models (\(n=200\) each).
We consider a single language
\(\ell\)\footnote{See SI for the matrix notation of this langauge.} with
\(|U|=6\) and \(|M|=4\) in which two of the utterances are ambiguous and
lower cost than the unambiguous utterances. (Note that use of this
particular language is not essential -- the results are broadly
generalizable to languages that contain ambiguity.)\par

\subsection{Speaker agents}\label{speaker-agents}

We consider three types of speaker models. Our \textit{Full pragmatics}
agent, models a speaker who reasons about her listener and also has
complete recall of the set of utterances in the discourse \(D\). This
speaker believes that the listener may not know the current topic
\(c_{current}\) at the start of the discrouse, but can infer it over the
discourse. We compare two baseline models. The first, a
\textit{Partial pragmatics} baseline describes a speaker who reasons
about a listener, but assumes they have no access to discourse history.
The second, a \textit{No pragmatics} speaker who does not consider a
listener at all, but produces utterances according to the underlying
langauge semantics (\(\ell\)) and topic probabilities
(\(p(M|C=c_{current}\)) (see SI section 3 for details).\par

\subsection{Hypotheses}\label{hypotheses}

We are interested in \textit{when} it is efficient to use low-cost,
ambiguous material in this discourse setting. A speaker strategy that is
mutually efficient for both agents should avoid ambiguity until
sufficient contextual information has accumulated. We should expect this
to be reflected in our \textit{Full pragmatics} model who reasons about
the listener and discourse history. By contrast, a speaker-optimal model
who does not consider the listener should greedily use ambiguous
utterances (\textit{No Pragmatics} model), while a listener-optimal
model should avoid ambiguity entirely (\textit{Partial pragmatics}
model).\par

\subsection{Results and Discussion}\label{results-and-discussion-1}

Figure 2, (A) shows the empirical probability that a speaker uses an
ambiguous utterance as a function of discourse position. The
\textit{No pragmatic} baseline uses ambiguous utterances frequently and
at a constant rate over the discourse and the \textit{Partial pragmatic}
baseline avoids ambiguous utterances entirely. But, the
\textit{Full pragmatic} model avoids ambiguous material only at the
start of the discourse, employing it increasingly as the discourse
proceeds. (C) tracks our CE objective for each model over the discourse.
Note that the objective decreases for all three models, primarily driven
by the listener updating his belief about the actual topic
(\(P(C=c_{current}|D)\)). However, the objective declines more quickly
under the \textit{Full} and \textit{Partial} pragmatic speakers as
listener agents are better able to infer the correct context.
Additionally, the difference in CE between the \textit{Full}- and
\textit{Partial pragmatic} models at the end of the discourse is driven
by the reduction in speaker costs described in (B). While speaker effort
remains constant in both \textit{No pragmatic} and
\textit{Partial pragmatic} baselines, effort declines in the
\textit{Full pragmatic} model as she increasingly relies on ambiguous
material later in the discourse.\par

In Simulation 2 we explored how ambiguity could be employed efficiently
during language use. We assumed that the listener did not know the
current context (topic) \textit{a priori}, but could infer it from the
discourse history. We examined speaker production behavior, tracking use
of low-cost, but ambiguous material over the discourse. Early in the
discourse the \textit{Full pragmatic} speaker avoided using ambiguous
material, only using the material later when the current topic was known
to the listener.

\section{General Discussion}\label{general-discussion}

How do the competing pressures of speakers and listeners give rise to
the distributional forms found in natural language? Zipf (1949) proposed
that the asymmetry between speaker and listener costs gives rise to a
range of properties at the level of the lexicon. We explored the
interactions of rational pragmatic agents as a framework for
understanding efficient language structure and use. We focused on an
argument on the communicative function of ambiguity (Piantadosi et al.,
2011), deriving a novel speaker-listener Cross-Entropy objective for
measuring the efficiency linguistic systems from first principles of
efficient language use. In a series of simulations we showed that
optimal languages are more likely to contain ambiguous material when
context is informative and that rational pragmatic agents will
\emph{use} ambiguous material efficiently.\par

A limitation of the current work is an analysis of exactly how this
objective compares to existing measures. For example, previous work has
described competing speaker-listener pressures in terms of a trade-off
of \textit{simplicity} and \textit{informativeness} (Kemp \& Regier,
2012) or \textit{expressivity} and \textit{compressibility} (Smith,
Tamariz, \& Kirby, 2013). Future work should assess the degree to which
we can derive the same properties as previous studies using our current
framework. In line with this idea, a primary focus of the current work
is to provide a case-study in the productivity of assessing
functionalist theories in repeated reference games between rational
pragmatic agents. In theory, this framework can serve as a domain
general tool to assess the range of functionalist theories examining
efficient language-structure and use.\par

\section{References}\label{references}

\setlength{\parindent}{-0.1in} \setlength{\leftskip}{0.125in} \noindent

\hypertarget{refs}{}
\hypertarget{ref-BennettGoodman2015a}{}
Bennett, E., \& Goodman, N. (2018). Extremely costly intensifiers are
stronger than quite costly ones. \emph{Cognition}.

\hypertarget{ref-Chomsky2002a}{}
Chomsky, N. (2002). An interview on minimilism. In \emph{N chomsky, on
nature and language}.

\hypertarget{ref-FerreriCancho2018a}{}
Ferrer-i-Cancho, R. (2018). Optimization models of natural
communication. \emph{Journal of Quantitative Linguistics}, \emph{25
(3)}, 207--237.

\hypertarget{ref-FrankGoodman2012a}{}
Frank, M., \& Goodman, N. (2012). Predicting pragmatic reasoning in
language games. \emph{Science}, \emph{336}, 998.

\hypertarget{ref-GenzelCharniak2002a}{}
Genzel, D., \& Charniak, E. (2002). Entropy rate constancy in text. In
\emph{Proceedings of the 40th annual meeting on association for
computational linguistics}.

\hypertarget{ref-GoodmanFrank2016a}{}
Goodman, N., \& Frank, M. (2016). Pragmatic language interpreation as
probabilistic inference. \emph{Trends in Cognitive Sciences},
\emph{20(11)}, 818--829.

\hypertarget{ref-Grice1975a}{}
Grice, P. H. (1975). Logic and conversation.

\hypertarget{ref-Hale2001a}{}
Hale, J. (2001). A probabilistic earley parser as a psycholinguistic
model. In \emph{Proceedings of the naacl}. 159-166.

\hypertarget{ref-HawkinsFrankeSmithGoodman2018a}{}
Hawkins, R., Franke, M., Smith, K., \& Goodman, N. (2018). Emerging
abstractions: Lexical conventions are shaped by communicative context.
In \emph{Proceedings of the 40th annual conference of the cognitive
science society}.

\hypertarget{ref-KempRegier2012a}{}
Kemp, C., \& Regier, T. (2012). Kinship categories across languages
reflect general communicative principles. \emph{Science}, \emph{336},
1049--1054.

\hypertarget{ref-KirbyGriffithsSmith2014a}{}
Kirby, S., Griffiths, T., \& Smith, K. (2014). Iterated learnign and the
evoluation of language. \emph{Current Opinion in Neurobiology},
\emph{28}, 108--114.

\hypertarget{ref-Levy2008a}{}
Levy, R. (2008). Expectation-based syntactic comprehension.
\emph{Cognition}, \emph{106(3)}, 1126--1177.

\hypertarget{ref-LevyJaeger2007a}{}
Levy, R., \& Jaeger, T. (2007). Speakers optimize information density
through syntactic reduction. In \emph{Proceedings of the twentieth
annual conference on neural information processing systems}.

\hypertarget{ref-Piantadosi2011a}{}
Piantadosi, S., Tily, H., \& Gibson, E. (2011). The communicative
function of ambiguity in language. \emph{Cognition}, \emph{122},
280--291.

\hypertarget{ref-RegierKempKay2015a}{}
Regier, T., Kemp, C., \& Kay, P. (2015). Word meanings across languages
support efficient communication. In B. M. \& W. O'Grady (Ed.), \emph{The
ahndbook of language emergence (pp. 237-263)}. Hoboken, NJ:
Wiley-Blackwell.

\hypertarget{ref-SmithTamarizKirby2013a}{}
Smith, K., Tamariz, M., \& Kirby, S. (2013). Linguistic structure is an
evolutionary trade-off between simplicity and expressivity. In
\emph{Proceedings of the 36th annual conference of the cognitive science
society}.

\hypertarget{ref-WasowPerforsBeaver2005a}{}
Wasow, T., Perfors, A., \& Beaver, D. (2005). The puzzle of ambiguity.
In CSLI (Ed.), \emph{Morphology and the web of grammar: Essays in memory
of steven g lapointe}. Stanford, CA: McGraw-Hill.

\hypertarget{ref-Zipf1949a}{}
Zipf, G. (1949). \emph{Human behavior and the principle of least
effort}. New York, NY: Prentice-Hall.

\bibliographystyle{apacite}


\end{document}
